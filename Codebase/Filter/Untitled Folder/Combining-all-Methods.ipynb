{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Methods - Basics - Correlations - Univariate ROC-AUC\n",
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 14)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"C:/Users/H A R I H A R A N/Desktop/sem 8/Heart Disease/heart.csv\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((242, 13), (61, 13))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['target'], axis=1),\n",
    "    data['target'],\n",
    "    test_size=0.2,\n",
    "    random_state=135)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep a copy of the dataset with all the variables\n",
    "# to measure the performance of machine learning models\n",
    "# at the end of the notebook\n",
    "\n",
    "X_train_original = X_train.copy()\n",
    "X_test_original = X_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove constant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((242, 13), (61, 13))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove constant features\n",
    "constant_features = [\n",
    "    feat for feat in X_train.columns if X_train[feat].std() == 0\n",
    "]\n",
    "\n",
    "X_train.drop(labels=constant_features, axis=1, inplace=True)\n",
    "X_test.drop(labels=constant_features, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove quasi-constant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove quasi-constant features\n",
    "# 0.1 indicates 99% of observations approximately\n",
    "sel = VarianceThreshold(threshold=0.01)  \n",
    "# fit finds the features with low variance\n",
    "sel.fit(X_train)  \n",
    "# how many not quasi-constant?\n",
    "sum(sel.get_support()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_keep = X_train.columns[sel.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((242, 13), (61, 13))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can then remove the features like this\n",
    "X_train = sel.transform(X_train)\n",
    "X_test = sel.transform(X_test)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn transformations lead to numpy arrays\n",
    "# here we transform the arrays back to dataframes\n",
    "# please be mindful of getting the columns assigned\n",
    "# correctly\n",
    "\n",
    "X_train= pd.DataFrame(X_train)\n",
    "X_train.columns = features_to_keep\n",
    "\n",
    "X_test= pd.DataFrame(X_test)\n",
    "X_test.columns = features_to_keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for duplicated features in the training set\n",
    "duplicated_feat = []\n",
    "for i in range(0, len(X_train.columns)):\n",
    "    if i % 10 == 0:  # this helps me understand how the loop is going\n",
    "        print(i)\n",
    "\n",
    "    col_1 = X_train.columns[i]\n",
    "\n",
    "    for col_2 in X_train.columns[i + 1:]:\n",
    "        if X_train[col_1].equals(X_train[col_2]):\n",
    "            duplicated_feat.append(col_2)\n",
    "            \n",
    "len(duplicated_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((242, 13), (61, 13))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove duplicated features\n",
    "X_train.drop(labels=duplicated_feat, axis=1, inplace=True)\n",
    "X_test.drop(labels=duplicated_feat, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep a copy of the dataset except constant and duplicated variables\n",
    "# to measure the performance of machine learning models\n",
    "# at the end of the notebook\n",
    "\n",
    "X_train_basic_filter = X_train.copy()\n",
    "X_test_basic_filter = X_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlated features:  0\n"
     ]
    }
   ],
   "source": [
    "# find and remove correlated features\n",
    "def correlation(dataset, threshold):\n",
    "    col_corr = set()  # Set of all the names of correlated columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
    "                colname = corr_matrix.columns[i]  # getting the name of column\n",
    "                col_corr.add(colname)\n",
    "    return col_corr\n",
    "\n",
    "corr_features = correlation(X_train, 0.8)\n",
    "print('correlated features: ', len(set(corr_features)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((242, 13), (61, 13))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removed correlated  features\n",
    "X_train.drop(labels=corr_features, axis=1, inplace=True)\n",
    "X_test.drop(labels=corr_features, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep a copy of the dataset at  this stage\n",
    "X_train_corr = X_train.copy()\n",
    "X_test_corr = X_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove features using univariate roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find important features using univariate roc-auc\n",
    "\n",
    "# loop to build a tree, make predictions and get the roc-auc\n",
    "# for each feature of the train set\n",
    "\n",
    "roc_values = []\n",
    "for feature in X_train.columns:\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(X_train[feature].fillna(0).to_frame(), y_train)\n",
    "    y_scored = clf.predict_proba(X_test[feature].fillna(0).to_frame())\n",
    "    roc_values.append(roc_auc_score(y_test, y_scored[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's add the variable names and order it for clearer visualisation\n",
    "roc_values = pd.Series(roc_values)\n",
    "roc_values.index = X_train.columns\n",
    "roc_values.sort_values(ascending=False).plot.bar(figsize=(20, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 13)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# by removing features with univariate roc_auc == 0.5\n",
    "# we remove another 30 features\n",
    "\n",
    "selected_feat = roc_values[roc_values>0.5]\n",
    "len(selected_feat), X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the performance in machine learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to build random forests and compare performance in train and test set\n",
    "\n",
    "def run_randomForests(X_train, X_test, y_train, y_test):\n",
    "    rf = RandomForestClassifier(n_estimators=200, random_state=39, max_depth=4)\n",
    "    rf.fit(X_train, y_train)\n",
    "    print('Train set')\n",
    "    pred = rf.predict_proba(X_train)\n",
    "    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "    print('Test set')\n",
    "    pred = rf.predict_proba(X_test)\n",
    "    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.9712809917355372\n",
      "Test set\n",
      "Random Forests roc-auc: 0.9880952380952381\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "run_randomForests(X_train_original,\n",
    "                  X_test_original,\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.9712809917355372\n",
      "Test set\n",
      "Random Forests roc-auc: 0.9880952380952381\n"
     ]
    }
   ],
   "source": [
    "# filter methods - basic\n",
    "run_randomForests(X_train_basic_filter,\n",
    "                  X_test_basic_filter,\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.9712809917355372\n",
      "Test set\n",
      "Random Forests roc-auc: 0.9880952380952381\n"
     ]
    }
   ],
   "source": [
    "# filter methods - correlation\n",
    "run_randomForests(X_train_corr,\n",
    "                  X_test_corr,\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.9671487603305785\n",
      "Test set\n",
      "Random Forests roc-auc: 0.9870129870129871\n"
     ]
    }
   ],
   "source": [
    "# filter methods - univariate roc-auc\n",
    "run_randomForests(X_train[selected_feat.index],\n",
    "                  X_test_corr[selected_feat.index],\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that removing constant, quasi-constant, duplicated, correlated and now **features with univariate roc-auc ==0.5** we still keep or even enhance the performance of the random forests (0.7985 vs 0.7900) at the time that we reduce the feature space dramatically (from 371 to 90).\n",
    "\n",
    "Let's have a look at the performance of logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to build logistic regression and compare performance in train and test set\n",
    "\n",
    "def run_logistic(X_train, X_test, y_train, y_test):\n",
    "    # function to train and test the performance of logistic regression\n",
    "    logit = LogisticRegression(random_state=44)\n",
    "    logit.fit(X_train, y_train)\n",
    "    print('Train set')\n",
    "    pred = logit.predict_proba(X_train)\n",
    "    print('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "    print('Test set')\n",
    "    pred = logit.predict_proba(X_test)\n",
    "    print('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Logistic Regression roc-auc: 0.9088154269972452\n",
      "Test set\n",
      "Logistic Regression roc-auc: 0.9707792207792207\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "scaler = StandardScaler().fit(X_train_original)\n",
    "\n",
    "run_logistic(scaler.transform(X_train_original),\n",
    "             scaler.transform(X_test_original),\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Logistic Regression roc-auc: 0.9088154269972452\n",
      "Test set\n",
      "Logistic Regression roc-auc: 0.9707792207792207\n"
     ]
    }
   ],
   "source": [
    "# filter methods - basic\n",
    "scaler = StandardScaler().fit(X_train_basic_filter)\n",
    "\n",
    "run_logistic(scaler.transform(X_train_basic_filter),\n",
    "             scaler.transform(X_test_basic_filter),\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Logistic Regression roc-auc: 0.9088154269972452\n",
      "Test set\n",
      "Logistic Regression roc-auc: 0.9707792207792207\n"
     ]
    }
   ],
   "source": [
    "# filter methods - correlation\n",
    "scaler = StandardScaler().fit(X_train_corr)\n",
    "\n",
    "run_logistic(scaler.transform(X_train_corr),\n",
    "             scaler.transform(X_test_corr),\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Logistic Regression roc-auc: 0.9088154269972453\n",
      "Test set\n",
      "Logistic Regression roc-auc: 0.9761904761904762\n"
     ]
    }
   ],
   "source": [
    "# filter methods - univariate roc-auc\n",
    "scaler = StandardScaler().fit(X_train[selected_feat.index])\n",
    "\n",
    "run_logistic(scaler.transform(X_train[selected_feat.index]),\n",
    "             scaler.transform(X_test_corr[selected_feat.index]),\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, removing constant, quasi-constant, duplicated, correlated and those with univariate roc_auc == 0.5, helped improve the performance of logistic regression (0.795 vs 0.794) at the time it reduced the feature space dramatically (371 to 90).\n",
    "\n",
    "We have now an equally or even slightly more predictive model that is at the time simpler, as it uses less than a third of the original features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
