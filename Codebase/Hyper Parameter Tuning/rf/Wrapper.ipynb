{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8892284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>241</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>110</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "0     63    1   3       145   233    1        0      150      0      2.3   \n",
       "1     37    1   2       130   250    0        1      187      0      3.5   \n",
       "2     41    0   1       130   204    0        0      172      0      1.4   \n",
       "3     56    1   1       120   236    0        1      178      0      0.8   \n",
       "4     57    0   0       120   354    0        1      163      1      0.6   \n",
       "..   ...  ...  ..       ...   ...  ...      ...      ...    ...      ...   \n",
       "298   57    0   0       140   241    0        1      123      1      0.2   \n",
       "299   45    1   3       110   264    0        1      132      0      1.2   \n",
       "300   68    1   0       144   193    1        1      141      0      3.4   \n",
       "301   57    1   0       130   131    0        1      115      1      1.2   \n",
       "302   57    0   1       130   236    0        0      174      0      0.0   \n",
       "\n",
       "     slope  ca  thal  target  \n",
       "0        0   0     1       1  \n",
       "1        0   0     2       1  \n",
       "2        2   0     2       1  \n",
       "3        2   0     2       1  \n",
       "4        2   0     2       1  \n",
       "..     ...  ..   ...     ...  \n",
       "298      1   0     3       0  \n",
       "299      1   0     3       0  \n",
       "300      1   2     3       0  \n",
       "301      1   1     3       0  \n",
       "302      1   1     2       0  \n",
       "\n",
       "[303 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"C:/Users/H A R I H A R A N/Desktop/sem 8/Heart Disease/heart.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebdd05b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "new = df[['age','sex',  'cp',   'trestbps',   'fbs',   'restecg',   'exang',   'slope',   'ca',   'thal', 'target']]\n",
    "\n",
    "Y = new['target']\n",
    "X = new.drop(['target'], axis = 1)\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(X, Y, test_size=0.20, random_state = 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "635ff870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.1639344262295\n"
     ]
    }
   ],
   "source": [
    "rf_Model = RandomForestClassifier() \n",
    "model = rf_Model.fit(x_train,y_train)\n",
    "print (model.score(x_test,y_test)*100)\n",
    "X = new.drop(['target'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "debbb42a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\H A R I H A R A N\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "1650 fits failed out of a total of 4950.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\H A R I H A R A N\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\H A R I H A R A N\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"C:\\Users\\H A R I H A R A N\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\H A R I H A R A N\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\H A R I H A R A N\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\H A R I H A R A N\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\H A R I H A R A N\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\H A R I H A R A N\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\H A R I H A R A N\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\H A R I H A R A N\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\H A R I H A R A N\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\H A R I H A R A N\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\H A R I H A R A N\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\H A R I H A R A N\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan 0.80166667\n",
      " 0.80583333 0.80583333 0.80566667 0.80583333 0.81       0.81833333\n",
      " 0.78916667 0.81       0.80166667        nan        nan        nan\n",
      "        nan        nan 0.81       0.785      0.79766667 0.80166667\n",
      " 0.78916667 0.81416667 0.81       0.7975     0.79333333 0.80166667\n",
      "        nan        nan        nan        nan        nan 0.8015\n",
      " 0.80983333 0.81       0.81816667 0.81       0.80983333 0.81416667\n",
      " 0.81       0.80166667 0.80183333        nan        nan        nan\n",
      "        nan        nan 0.82666667 0.80583333 0.80583333 0.80983333\n",
      " 0.78516667 0.814      0.7975     0.8015     0.81833333 0.80166667\n",
      "        nan        nan        nan        nan        nan 0.80583333\n",
      " 0.80583333 0.7975     0.80583333 0.81       0.80583333 0.7975\n",
      " 0.81       0.80583333 0.80166667        nan        nan        nan\n",
      "        nan        nan 0.81       0.785      0.80166667 0.80166667\n",
      " 0.7975     0.80166667 0.8015     0.80583333 0.80166667 0.80566667\n",
      "        nan        nan        nan        nan        nan 0.7975\n",
      " 0.80583333 0.806      0.80583333 0.81416667 0.81416667 0.80583333\n",
      " 0.80166667 0.814      0.79333333        nan        nan        nan\n",
      "        nan        nan 0.80983333 0.79333333 0.80166667 0.81416667\n",
      " 0.80583333 0.78933333 0.82233333 0.7975     0.80166667 0.80166667\n",
      "        nan        nan        nan        nan        nan 0.79333333\n",
      " 0.7975     0.80183333 0.7975     0.81       0.7975     0.806\n",
      " 0.814      0.80583333 0.7935            nan        nan        nan\n",
      "        nan        nan 0.83066667 0.81       0.80166667 0.81416667\n",
      " 0.81816667 0.814      0.81816667 0.835      0.80983333 0.80166667\n",
      "        nan        nan        nan        nan        nan 0.818\n",
      " 0.81416667 0.81       0.80166667 0.78916667 0.806      0.7975\n",
      " 0.79333333 0.7975     0.80166667        nan        nan        nan\n",
      "        nan        nan 0.81       0.80583333 0.81416667 0.80566667\n",
      " 0.80583333 0.80566667 0.80583333 0.79316667 0.79333333 0.80583333\n",
      "        nan        nan        nan        nan        nan 0.80583333\n",
      " 0.80166667 0.814      0.81416667 0.80166667 0.81       0.8015\n",
      " 0.8225     0.81       0.80566667        nan        nan        nan\n",
      "        nan        nan 0.80166667 0.81416667 0.8055     0.7975\n",
      " 0.7975     0.80566667 0.789      0.80583333 0.80166667 0.81416667\n",
      "        nan        nan        nan        nan        nan 0.8055\n",
      " 0.80566667 0.80183333 0.81816667 0.7975     0.79333333 0.78933333\n",
      " 0.806      0.80583333 0.81              nan        nan        nan\n",
      "        nan        nan 0.8225     0.81416667 0.81016667 0.79333333\n",
      " 0.80166667 0.81433333 0.81       0.80183333 0.80583333 0.81416667\n",
      "        nan        nan        nan        nan        nan 0.7935\n",
      " 0.80583333 0.81       0.80583333 0.7975     0.82233333 0.80166667\n",
      " 0.781      0.80583333 0.80166667        nan        nan        nan\n",
      "        nan        nan 0.81416667 0.80583333 0.80166667 0.80566667\n",
      " 0.7975     0.789      0.79783333 0.81833333 0.80166667 0.80166667\n",
      "        nan        nan        nan        nan        nan 0.814\n",
      " 0.80583333 0.81       0.81       0.80583333 0.7935     0.81\n",
      " 0.80583333 0.81       0.80583333        nan        nan        nan\n",
      "        nan        nan 0.81416667 0.7975     0.80583333 0.81\n",
      " 0.79333333 0.83483333 0.81       0.81416667 0.80583333 0.80583333\n",
      "        nan        nan        nan        nan        nan 0.8185\n",
      " 0.81833333 0.806      0.80583333 0.7975     0.8185     0.7975\n",
      " 0.81       0.80583333 0.80583333        nan        nan        nan\n",
      "        nan        nan 0.81816667 0.78516667 0.80166667 0.81\n",
      " 0.81       0.82233333 0.8015     0.79333333 0.814      0.80583333\n",
      "        nan        nan        nan        nan        nan 0.81416667\n",
      " 0.81       0.7975     0.79333333 0.80166667 0.7975     0.80166667\n",
      " 0.7975     0.80166667 0.81              nan        nan        nan\n",
      "        nan        nan 0.80183333 0.81016667 0.80183333 0.81816667\n",
      " 0.80183333 0.8265     0.80583333 0.8015     0.81       0.8015\n",
      "        nan        nan        nan        nan        nan 0.81816667\n",
      " 0.80583333 0.80166667 0.81016667 0.81       0.80983333 0.81416667\n",
      " 0.81416667 0.80983333 0.80166667        nan        nan        nan\n",
      "        nan        nan 0.8225     0.789      0.81816667 0.81416667\n",
      " 0.81       0.7975     0.81       0.8015     0.81416667 0.7975\n",
      "        nan        nan        nan        nan        nan 0.81416667\n",
      " 0.80583333 0.7975     0.8265     0.8225     0.80166667 0.78933333\n",
      " 0.81       0.80166667 0.80566667        nan        nan        nan\n",
      "        nan        nan 0.81416667 0.7975     0.80583333 0.814\n",
      " 0.79333333 0.80583333 0.81       0.80583333 0.8015     0.80166667\n",
      "        nan        nan        nan        nan        nan 0.79333333\n",
      " 0.81       0.80583333 0.80166667 0.81416667 0.8015     0.7975\n",
      " 0.7975     0.81       0.81416667        nan        nan        nan\n",
      "        nan        nan 0.7975     0.80183333 0.82633333 0.806\n",
      " 0.79333333 0.81833333 0.8185     0.81833333 0.80583333 0.81\n",
      "        nan        nan        nan        nan        nan 0.79333333\n",
      " 0.78916667 0.80583333 0.82233333 0.81416667 0.78933333 0.80583333\n",
      " 0.80583333 0.79333333 0.80583333        nan        nan        nan\n",
      "        nan        nan 0.80583333 0.81       0.785      0.80166667\n",
      " 0.78916667 0.79333333 0.80583333 0.80566667 0.80166667 0.80166667\n",
      "        nan        nan        nan        nan        nan 0.79366667\n",
      " 0.81       0.80166667 0.81       0.8015     0.81416667 0.80166667\n",
      " 0.81416667 0.81       0.81      ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=830, max_features='sqrt', min_samples_split=3,\n",
       "                       n_estimators=300)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_Model = RandomForestClassifier() \n",
    "model = rf_Model.fit(x_train,y_train)\n",
    "forest_params = [{'max_depth': [800,810,820,830,840,850,860,870,880,890,900],\n",
    "                  'max_features': ['sqrt'],\n",
    "                  'criterion' : ['gini'] ,\n",
    "                  'min_samples_leaf': [1,2,3],\n",
    "                  'min_samples_split': [1,2,3],\n",
    "                  'n_estimators':[100,200,300,400,500]}]\n",
    "clf = GridSearchCV(model, forest_params, cv = 10, scoring='accuracy')\n",
    "best_clf = clf.fit(x_train,y_train)\n",
    "best_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f066267e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.80327868852459\n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(X, Y, test_size=0.20, random_state = 128)\n",
    "rf_Model = RandomForestClassifier(max_depth=830, max_features='sqrt', min_samples_split=3,\n",
    "                       n_estimators=300) \n",
    "model = rf_Model.fit(x_train,y_train)\n",
    "print (model.score(x_test,y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5e3aa54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "[CV] END criterion=gini, max_depth=650, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=650, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=650, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=650, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.3s\n",
      "[CV] END criterion=gini, max_depth=650, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=650, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=650, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=650, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=650, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.3s\n",
      "[CV] END criterion=gini, max_depth=650, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=650, max_features=log2, min_samples_leaf=2, min_samples_split=4, n_estimators=300; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=650, max_features=log2, min_samples_leaf=2, min_samples_split=4, n_estimators=300; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=650, max_features=log2, min_samples_leaf=2, min_samples_split=4, n_estimators=300; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=650, max_features=log2, min_samples_leaf=2, min_samples_split=4, n_estimators=300; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=650, max_features=log2, min_samples_leaf=2, min_samples_split=4, n_estimators=300; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=650, max_features=log2, min_samples_leaf=2, min_samples_split=4, n_estimators=300; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=650, max_features=log2, min_samples_leaf=2, min_samples_split=4, n_estimators=300; total time=   0.3s\n",
      "[CV] END criterion=gini, max_depth=650, max_features=log2, min_samples_leaf=2, min_samples_split=4, n_estimators=300; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=650, max_features=log2, min_samples_leaf=2, min_samples_split=4, n_estimators=300; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=650, max_features=log2, min_samples_leaf=2, min_samples_split=4, n_estimators=300; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=620, max_features=log2, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=620, max_features=log2, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=620, max_features=log2, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=620, max_features=log2, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=620, max_features=log2, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=620, max_features=log2, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=620, max_features=log2, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=620, max_features=log2, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=620, max_features=log2, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=620, max_features=log2, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=700, max_features=log2, min_samples_leaf=3, min_samples_split=4, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=700, max_features=log2, min_samples_leaf=3, min_samples_split=4, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=700, max_features=log2, min_samples_leaf=3, min_samples_split=4, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=700, max_features=log2, min_samples_leaf=3, min_samples_split=4, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=700, max_features=log2, min_samples_leaf=3, min_samples_split=4, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=700, max_features=log2, min_samples_leaf=3, min_samples_split=4, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=700, max_features=log2, min_samples_leaf=3, min_samples_split=4, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=700, max_features=log2, min_samples_leaf=3, min_samples_split=4, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=700, max_features=log2, min_samples_leaf=3, min_samples_split=4, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=700, max_features=log2, min_samples_leaf=3, min_samples_split=4, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=640, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END criterion=gini, max_depth=640, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END criterion=gini, max_depth=640, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END criterion=gini, max_depth=640, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END criterion=gini, max_depth=640, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END criterion=gini, max_depth=640, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END criterion=gini, max_depth=640, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END criterion=gini, max_depth=640, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END criterion=gini, max_depth=640, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END criterion=gini, max_depth=640, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=630, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=630, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=630, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300; total time=   0.3s\n",
      "[CV] END criterion=gini, max_depth=630, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=630, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300; total time=   0.3s\n",
      "[CV] END criterion=gini, max_depth=630, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=630, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=630, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=630, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=630, max_features=sqrt, min_samples_leaf=3, min_samples_split=2, n_estimators=300; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=600, max_features=sqrt, min_samples_leaf=3, min_samples_split=1, n_estimators=400; total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END criterion=gini, max_depth=600, max_features=sqrt, min_samples_leaf=3, min_samples_split=1, n_estimators=400; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=600, max_features=sqrt, min_samples_leaf=3, min_samples_split=1, n_estimators=400; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=600, max_features=sqrt, min_samples_leaf=3, min_samples_split=1, n_estimators=400; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=600, max_features=sqrt, min_samples_leaf=3, min_samples_split=1, n_estimators=400; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=600, max_features=sqrt, min_samples_leaf=3, min_samples_split=1, n_estimators=400; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=600, max_features=sqrt, min_samples_leaf=3, min_samples_split=1, n_estimators=400; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=600, max_features=sqrt, min_samples_leaf=3, min_samples_split=1, n_estimators=400; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=600, max_features=sqrt, min_samples_leaf=3, min_samples_split=1, n_estimators=400; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=600, max_features=sqrt, min_samples_leaf=3, min_samples_split=1, n_estimators=400; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=700, max_features=log2, min_samples_leaf=2, min_samples_split=4, n_estimators=300; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=700, max_features=log2, min_samples_leaf=2, min_samples_split=4, n_estimators=300; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=700, max_features=log2, min_samples_leaf=2, min_samples_split=4, n_estimators=300; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=700, max_features=log2, min_samples_leaf=2, min_samples_split=4, n_estimators=300; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=700, max_features=log2, min_samples_leaf=2, min_samples_split=4, n_estimators=300; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=700, max_features=log2, min_samples_leaf=2, min_samples_split=4, n_estimators=300; total time=   0.3s\n",
      "[CV] END criterion=gini, max_depth=700, max_features=log2, min_samples_leaf=2, min_samples_split=4, n_estimators=300; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=700, max_features=log2, min_samples_leaf=2, min_samples_split=4, n_estimators=300; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=700, max_features=log2, min_samples_leaf=2, min_samples_split=4, n_estimators=300; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=700, max_features=log2, min_samples_leaf=2, min_samples_split=4, n_estimators=300; total time=   0.2s\n",
      "[CV] END criterion=gini, max_depth=620, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END criterion=gini, max_depth=620, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END criterion=gini, max_depth=620, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END criterion=gini, max_depth=620, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END criterion=gini, max_depth=620, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END criterion=gini, max_depth=620, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END criterion=gini, max_depth=620, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END criterion=gini, max_depth=620, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END criterion=gini, max_depth=620, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END criterion=gini, max_depth=620, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END criterion=gini, max_depth=610, max_features=sqrt, min_samples_leaf=3, min_samples_split=1, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=610, max_features=sqrt, min_samples_leaf=3, min_samples_split=1, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=610, max_features=sqrt, min_samples_leaf=3, min_samples_split=1, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=610, max_features=sqrt, min_samples_leaf=3, min_samples_split=1, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=610, max_features=sqrt, min_samples_leaf=3, min_samples_split=1, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=610, max_features=sqrt, min_samples_leaf=3, min_samples_split=1, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=610, max_features=sqrt, min_samples_leaf=3, min_samples_split=1, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=610, max_features=sqrt, min_samples_leaf=3, min_samples_split=1, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=610, max_features=sqrt, min_samples_leaf=3, min_samples_split=1, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=610, max_features=sqrt, min_samples_leaf=3, min_samples_split=1, n_estimators=200; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\H A R I H A R A N\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "20 fits failed out of a total of 100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\H A R I H A R A N\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\H A R I H A R A N\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"C:\\Users\\H A R I H A R A N\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\H A R I H A R A N\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\H A R I H A R A N\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\H A R I H A R A N\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\H A R I H A R A N\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\H A R I H A R A N\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\H A R I H A R A N\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\H A R I H A R A N\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\H A R I H A R A N\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\H A R I H A R A N\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\H A R I H A R A N\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\H A R I H A R A N\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.8265     0.83466667 0.8265     0.8265     0.83083333 0.8265\n",
      "        nan 0.83483333 0.839             nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=620, max_features='sqrt', min_samples_leaf=2,\n",
       "                       n_estimators=200)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RANDOMIZED SEARCH CV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "x_train,x_test,y_train,y_test = train_test_split(X, Y, test_size=0.20, random_state = 135)\n",
    "forest_params = [{'max_depth': [600,610,620,630,640,650,660,670,680,690,700],\n",
    "                  'max_features': ['sqrt','log2'],\n",
    "                  'criterion' : ['gini'] ,\n",
    "                  'min_samples_leaf': [1,2,3],\n",
    "                  'min_samples_split': [1,2,4],\n",
    "                  'n_estimators':[100,200,300,400]}]\n",
    "rf_RandomGrid = RandomizedSearchCV(estimator = rf_Model, param_distributions = forest_params, cv = 10, verbose=2, scoring = 'accuracy')\n",
    "best_clf = rf_RandomGrid.fit(x_train,y_train)\n",
    "best_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "848dd4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.80327868852459\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(X, Y, test_size=0.20, random_state = 65)\n",
    "rf_Model = RandomForestClassifier(max_depth=620, max_features='sqrt', min_samples_leaf=2,\n",
    "                       n_estimators=200)\n",
    "model = rf_Model.fit(x_train,y_train)\n",
    "print (model.score(x_test,y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1611dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.1639344262295 1\n",
      "90.1639344262295 2\n",
      "91.80327868852459 3\n",
      "91.80327868852459 4\n",
      "90.1639344262295 5\n",
      "91.80327868852459 6\n",
      "91.80327868852459 7\n",
      "91.80327868852459 8\n",
      "90.1639344262295 9\n",
      "90.1639344262295 10\n",
      "91.80327868852459 11\n",
      "90.1639344262295 12\n",
      "91.80327868852459 13\n",
      "90.1639344262295 14\n",
      "90.1639344262295 15\n",
      "91.80327868852459 16\n",
      "91.80327868852459 17\n",
      "90.1639344262295 18\n",
      "90.1639344262295 19\n",
      "91.80327868852459 20\n",
      "91.80327868852459 21\n",
      "90.1639344262295 22\n",
      "91.80327868852459 23\n",
      "93.44262295081968 24\n",
      "90.1639344262295 25\n",
      "91.80327868852459 26\n",
      "91.80327868852459 27\n",
      "90.1639344262295 28\n",
      "90.1639344262295 29\n",
      "91.80327868852459 30\n",
      "91.80327868852459 31\n",
      "91.80327868852459 32\n",
      "91.80327868852459 33\n",
      "90.1639344262295 34\n",
      "91.80327868852459 35\n",
      "91.80327868852459 36\n",
      "90.1639344262295 37\n",
      "90.1639344262295 38\n",
      "91.80327868852459 39\n",
      "91.80327868852459 40\n",
      "91.80327868852459 41\n",
      "91.80327868852459 42\n",
      "90.1639344262295 43\n",
      "91.80327868852459 44\n",
      "91.80327868852459 45\n",
      "90.1639344262295 46\n",
      "91.80327868852459 47\n",
      "91.80327868852459 48\n",
      "90.1639344262295 49\n",
      "91.80327868852459 50\n",
      "90.1639344262295 51\n",
      "91.80327868852459 52\n",
      "90.1639344262295 53\n",
      "91.80327868852459 54\n",
      "91.80327868852459 55\n",
      "90.1639344262295 56\n",
      "90.1639344262295 57\n",
      "90.1639344262295 58\n",
      "91.80327868852459 59\n",
      "90.1639344262295 60\n",
      "91.80327868852459 61\n",
      "91.80327868852459 62\n",
      "90.1639344262295 63\n",
      "90.1639344262295 64\n",
      "91.80327868852459 65\n",
      "90.1639344262295 66\n",
      "91.80327868852459 67\n",
      "91.80327868852459 68\n",
      "91.80327868852459 69\n",
      "91.80327868852459 70\n",
      "91.80327868852459 71\n",
      "91.80327868852459 72\n",
      "91.80327868852459 73\n",
      "91.80327868852459 74\n",
      "91.80327868852459 75\n",
      "90.1639344262295 76\n",
      "90.1639344262295 77\n",
      "91.80327868852459 78\n",
      "91.80327868852459 79\n",
      "91.80327868852459 80\n",
      "91.80327868852459 81\n",
      "90.1639344262295 82\n",
      "90.1639344262295 83\n",
      "91.80327868852459 84\n",
      "91.80327868852459 85\n",
      "91.80327868852459 86\n",
      "91.80327868852459 87\n",
      "90.1639344262295 88\n",
      "90.1639344262295 89\n",
      "90.1639344262295 90\n",
      "90.1639344262295 91\n",
      "91.80327868852459 92\n",
      "91.80327868852459 93\n",
      "90.1639344262295 94\n",
      "91.80327868852459 95\n",
      "90.1639344262295 96\n",
      "90.1639344262295 97\n",
      "91.80327868852459 98\n",
      "90.1639344262295 99\n",
      "90.1639344262295 100\n",
      "91.80327868852459 101\n",
      "90.1639344262295 102\n",
      "91.80327868852459 103\n",
      "90.1639344262295 104\n",
      "91.80327868852459 105\n",
      "91.80327868852459 106\n",
      "90.1639344262295 107\n",
      "91.80327868852459 108\n",
      "90.1639344262295 109\n",
      "91.80327868852459 110\n",
      "91.80327868852459 111\n",
      "90.1639344262295 112\n",
      "91.80327868852459 113\n",
      "90.1639344262295 114\n",
      "91.80327868852459 115\n",
      "91.80327868852459 116\n",
      "91.80327868852459 117\n",
      "91.80327868852459 118\n",
      "90.1639344262295 119\n",
      "90.1639344262295 120\n",
      "91.80327868852459 121\n",
      "91.80327868852459 122\n",
      "91.80327868852459 123\n",
      "90.1639344262295 124\n",
      "91.80327868852459 125\n",
      "91.80327868852459 126\n",
      "90.1639344262295 127\n",
      "91.80327868852459 128\n",
      "90.1639344262295 129\n",
      "90.1639344262295 130\n",
      "90.1639344262295 131\n",
      "90.1639344262295 132\n",
      "90.1639344262295 133\n",
      "91.80327868852459 134\n",
      "91.80327868852459 135\n",
      "90.1639344262295 136\n",
      "90.1639344262295 137\n",
      "91.80327868852459 138\n",
      "91.80327868852459 139\n",
      "90.1639344262295 140\n",
      "90.1639344262295 141\n",
      "90.1639344262295 142\n",
      "90.1639344262295 143\n",
      "90.1639344262295 144\n",
      "90.1639344262295 145\n",
      "91.80327868852459 146\n",
      "93.44262295081968 147\n",
      "90.1639344262295 148\n",
      "91.80327868852459 149\n",
      "90.1639344262295 150\n",
      "90.1639344262295 151\n",
      "91.80327868852459 152\n",
      "90.1639344262295 153\n",
      "90.1639344262295 154\n",
      "90.1639344262295 155\n",
      "91.80327868852459 156\n",
      "91.80327868852459 157\n",
      "90.1639344262295 158\n",
      "91.80327868852459 159\n",
      "91.80327868852459 160\n",
      "91.80327868852459 161\n",
      "93.44262295081968 162\n",
      "91.80327868852459 163\n",
      "91.80327868852459 164\n",
      "90.1639344262295 165\n",
      "90.1639344262295 166\n",
      "91.80327868852459 167\n",
      "91.80327868852459 168\n",
      "91.80327868852459 169\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m x_train,x_test,y_train,y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, Y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.20\u001b[39m, random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m65\u001b[39m)\n\u001b[0;32m      3\u001b[0m rf_Model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m620\u001b[39m, max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msqrt\u001b[39m\u001b[38;5;124m'\u001b[39m, min_samples_leaf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m      4\u001b[0m                    n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mrf_Model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m (model\u001b[38;5;241m.\u001b[39mscore(x_test,y_test)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m, i)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:450\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    439\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    442\u001b[0m ]\n\u001b[0;32m    444\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 450\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_joblib_parallel_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:185\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    183\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 185\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:937\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[0;32m    900\u001b[0m     \u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, X_idx_sorted\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    901\u001b[0m ):\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \n\u001b[0;32m    904\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 937\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_idx_sorted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_idx_sorted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    944\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:203\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_classification:\n\u001b[1;32m--> 203\u001b[0m     \u001b[43mcheck_classification_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcopy(y)\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\multiclass.py:189\u001b[0m, in \u001b[0;36mcheck_classification_targets\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_classification_targets\u001b[39m(y):\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;124;03m\"\"\"Ensure that target y is of a non-regression type.\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03m    Only the following target types (as defined in type_of_target) are allowed:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;124;03m    y : array-like\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 189\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m \u001b[43mtype_of_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    196\u001b[0m     ]:\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown label type: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\multiclass.py:327\u001b[0m, in \u001b[0;36mtype_of_target\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    324\u001b[0m     _assert_all_finite(y)\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontinuous\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m suffix\n\u001b[1;32m--> 327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m suffix  \u001b[38;5;66;03m# [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36munique\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:272\u001b[0m, in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[0;32m    270\u001b[0m ar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(ar)\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 272\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[0;32m    275\u001b[0m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:333\u001b[0m, in \u001b[0;36m_unique1d\u001b[1;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[0;32m    331\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar[perm]\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 333\u001b[0m     \u001b[43mar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    334\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar\n\u001b[0;32m    335\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(aux\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mbool_)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1,200):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(X, Y, test_size=0.20, random_state = 65)\n",
    "    rf_Model = RandomForestClassifier(max_depth=620, max_features='sqrt', min_samples_leaf=2,\n",
    "                       n_estimators=200)\n",
    "    model = rf_Model.fit(x_train,y_train)\n",
    "    print (model.score(x_test,y_test)*100, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f18bd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('criterion', 'gini'), ('max_depth', 881), ('max_features', 'log2'), ('n_estimators', 764)])\n",
      "Accuracy:0.8098251028806583\n"
     ]
    }
   ],
   "source": [
    "from skopt import Optimizer\n",
    "from skopt import BayesSearchCV \n",
    "from skopt.space import Real, Categorical, Integer\n",
    "# Define the hyperparameter configuration space\n",
    "rf_params = {\n",
    "    'n_estimators': np.arange(100,1100),\n",
    "    \"max_features\":['log2','sqrt'],\n",
    "    'max_depth': np.arange(600,1000),\n",
    "    \"criterion\":['gini','entropy']\n",
    "}\n",
    "clf = RandomForestClassifier(random_state=135)\n",
    "Bayes = BayesSearchCV(clf, rf_params,cv=3,n_iter=20, n_jobs=-1,scoring='accuracy')\n",
    "#number of iterations is set to 20, you can increase this number if time permits\n",
    "Bayes.fit(x_train, y_train)\n",
    "print(Bayes.best_params_)\n",
    "bclf = Bayes.best_estimator_\n",
    "print(\"Accuracy:\"+ str(Bayes.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c3c595ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.1639344262295\n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(X, Y, test_size=0.20, random_state = 65)\n",
    "rf_Model = RandomForestClassifier(max_depth=881, max_features='log2', criterion = 'gini',\n",
    "                       n_estimators=764)\n",
    "model = rf_Model.fit(x_train,y_train)\n",
    "print (model.score(x_test,y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "35bd021d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5525685c81c46c49205882c624df24f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/60 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.810204081632653\tRandomForestClassifier(input_matrix, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_depth=834, RandomForestClassifier__max_features=sqrt, RandomForestClassifier__min_samples_leaf=2, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=679)\n",
      "\n",
      "-2\t0.8392857142857142\tRandomForestClassifier(RandomForestClassifier(input_matrix, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_depth=660, RandomForestClassifier__max_features=log2, RandomForestClassifier__min_samples_leaf=2, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=871), RandomForestClassifier__criterion=gini, RandomForestClassifier__max_depth=661, RandomForestClassifier__max_features=sqrt, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=358)\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.810204081632653\tRandomForestClassifier(input_matrix, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_depth=834, RandomForestClassifier__max_features=sqrt, RandomForestClassifier__min_samples_leaf=2, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=679)\n",
      "\n",
      "-2\t0.8392857142857142\tRandomForestClassifier(RandomForestClassifier(input_matrix, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_depth=660, RandomForestClassifier__max_features=log2, RandomForestClassifier__min_samples_leaf=2, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=871), RandomForestClassifier__criterion=gini, RandomForestClassifier__max_depth=661, RandomForestClassifier__max_features=sqrt, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=358)\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8142006802721088\tRandomForestClassifier(input_matrix, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_depth=652, RandomForestClassifier__max_features=log2, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=2, RandomForestClassifier__n_estimators=446)\n",
      "\n",
      "-2\t0.8392857142857142\tRandomForestClassifier(RandomForestClassifier(input_matrix, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_depth=660, RandomForestClassifier__max_features=log2, RandomForestClassifier__min_samples_leaf=2, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=871), RandomForestClassifier__criterion=gini, RandomForestClassifier__max_depth=661, RandomForestClassifier__max_features=sqrt, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=358)\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8142006802721088\tRandomForestClassifier(input_matrix, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_depth=652, RandomForestClassifier__max_features=log2, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=2, RandomForestClassifier__n_estimators=446)\n",
      "\n",
      "-2\t0.8392857142857142\tRandomForestClassifier(RandomForestClassifier(input_matrix, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_depth=660, RandomForestClassifier__max_features=log2, RandomForestClassifier__min_samples_leaf=2, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=871), RandomForestClassifier__criterion=gini, RandomForestClassifier__max_depth=661, RandomForestClassifier__max_features=sqrt, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=358)\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8142006802721088\tRandomForestClassifier(input_matrix, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_depth=652, RandomForestClassifier__max_features=log2, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=2, RandomForestClassifier__n_estimators=446)\n",
      "\n",
      "-2\t0.8392857142857142\tRandomForestClassifier(RandomForestClassifier(input_matrix, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_depth=660, RandomForestClassifier__max_features=log2, RandomForestClassifier__min_samples_leaf=2, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=871), RandomForestClassifier__criterion=gini, RandomForestClassifier__max_depth=661, RandomForestClassifier__max_features=sqrt, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=358)\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8142006802721088\tRandomForestClassifier(input_matrix, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_depth=652, RandomForestClassifier__max_features=log2, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=2, RandomForestClassifier__n_estimators=446)\n",
      "\n",
      "-2\t0.8392857142857142\tRandomForestClassifier(RandomForestClassifier(input_matrix, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_depth=660, RandomForestClassifier__max_features=log2, RandomForestClassifier__min_samples_leaf=2, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=871), RandomForestClassifier__criterion=gini, RandomForestClassifier__max_depth=661, RandomForestClassifier__max_features=sqrt, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=358)\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8142006802721088\tRandomForestClassifier(input_matrix, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_depth=652, RandomForestClassifier__max_features=log2, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=2, RandomForestClassifier__n_estimators=446)\n",
      "\n",
      "-2\t0.8392857142857142\tRandomForestClassifier(RandomForestClassifier(input_matrix, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_depth=660, RandomForestClassifier__max_features=log2, RandomForestClassifier__min_samples_leaf=2, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=871), RandomForestClassifier__criterion=gini, RandomForestClassifier__max_depth=661, RandomForestClassifier__max_features=sqrt, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=358)\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.8142006802721088\tRandomForestClassifier(input_matrix, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_depth=652, RandomForestClassifier__max_features=log2, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=2, RandomForestClassifier__n_estimators=446)\n",
      "\n",
      "-2\t0.8392857142857142\tRandomForestClassifier(RandomForestClassifier(input_matrix, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_depth=660, RandomForestClassifier__max_features=log2, RandomForestClassifier__min_samples_leaf=2, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=871), RandomForestClassifier__criterion=gini, RandomForestClassifier__max_depth=661, RandomForestClassifier__max_features=sqrt, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=358)\n",
      "\n",
      "The optimized pipeline was not improved after evaluating 5 more generations. Will end the optimization process.\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TPOTClassifier(config_dict={'sklearn.ensemble.RandomForestClassifier': {'criterion': ['gini',\n",
       "                                                                                      'entropy'],\n",
       "                                                                        'max_depth': array([600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612,\n",
       "       613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625,\n",
       "       626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638,\n",
       "       639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651,\n",
       "       652, 653, 654, 655, 656, 6...\n",
       "       1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056,\n",
       "       1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067,\n",
       "       1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078,\n",
       "       1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089,\n",
       "       1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099])}},\n",
       "               early_stop=5, generations=10, offspring_size=5,\n",
       "               population_size=10, scoring='accuracy', verbosity=3)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tpot import TPOTClassifier\n",
    "# Define the hyperparameter configuration space\n",
    "parameters = {\n",
    "        'n_estimators': np.arange(100,1100),\n",
    "    \"max_features\":['log2','sqrt'],\n",
    "    'max_depth': np.arange(600,1000),\n",
    "    \"min_samples_split\":range(2,5),\n",
    "    \"min_samples_leaf\":range(1,4),\n",
    "    \"criterion\":['gini','entropy']\n",
    "             }\n",
    "# Set the hyperparameters of GA                 \n",
    "ga2 = TPOTClassifier(generations= 10, population_size= 10, offspring_size= 5,\n",
    "                                 verbosity= 3, early_stop= 5,\n",
    "                                 config_dict=\n",
    "                                 {'sklearn.ensemble.RandomForestClassifier': parameters}, \n",
    "                                 cv = 5, scoring = 'accuracy')\n",
    "ga2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f2aec97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.80327868852459\n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(X, Y, test_size=0.20, random_state = 65)\n",
    "rf_Model = RandomForestClassifier(max_depth=661, max_features='sqrt', criterion = 'gini',\n",
    "                       n_estimators=358,min_samples_split=4,min_samples_leaf=1)\n",
    "model = rf_Model.fit(x_train,y_train)\n",
    "print (model.score(x_test,y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4ad03174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn-genetic in c:\\users\\h a r i h a r a n\\anaconda3\\lib\\site-packages (0.5.1)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: deap>=1.0.2 in c:\\users\\h a r i h a r a n\\anaconda3\\lib\\site-packages (from sklearn-genetic) (1.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\h a r i h a r a n\\anaconda3\\lib\\site-packages (from sklearn-genetic) (1.21.5)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\h a r i h a r a n\\anaconda3\\lib\\site-packages (from sklearn-genetic) (0.70.14)\n",
      "Requirement already satisfied: scikit-learn>=0.23 in c:\\users\\h a r i h a r a n\\anaconda3\\lib\\site-packages (from sklearn-genetic) (1.0.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\h a r i h a r a n\\anaconda3\\lib\\site-packages (from scikit-learn>=0.23->sklearn-genetic) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\h a r i h a r a n\\anaconda3\\lib\\site-packages (from scikit-learn>=0.23->sklearn-genetic) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\h a r i h a r a n\\anaconda3\\lib\\site-packages (from scikit-learn>=0.23->sklearn-genetic) (1.7.3)\n",
      "Requirement already satisfied: dill>=0.3.6 in c:\\users\\h a r i h a r a n\\anaconda3\\lib\\site-packages (from multiprocess->sklearn-genetic) (0.3.6)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn-genetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5c2278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
